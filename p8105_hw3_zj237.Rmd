---
title: "p8105_hw3_zj2357"
author: "Zekai Jin (zj2357)"
date: "2022-10-10"
output: github_document
---

First of all, we should include the libraries we are using.\
```{r init, message=FALSE}
library(tidyverse)
library(lubridate)
library(forcats)
library(ggridges)
library(p8105.datasets)
knitr::opts_chunk$set(message  = FALSE,
                      warning = FALSE)
```

# Problem 1\
```{r}
data("instacart")
head(instacart,5)
```
This dataset consists of `r nrow(instacart)` records, each record represent an item purchased by a user within an order. `order_id` shows that there are `r nrow(distinct(instacart, order_id))` different orders. the other variables shows the time of the order, the name of the item and their categories. \


To analyze the aisles, use the following code\

```{r}

aisle_group = 
  instacart %>%
  group_by(aisle) %>%
  summarise(n_obs =  n()) %>%
  arrange(-n_obs)
```


Thus, there are `r nrow(aisle_group)` different aisles. The most popular aisle is `r aisle_group$aisle[1]`.\
Then, we make a plot of aisles with more than 10000 items ordered.\

```{r}
aisle_group %>%
  mutate(aisle = fct_reorder(aisle, -n_obs)) %>%
  filter(n_obs > 10000) %>%
  ggplot(aes(x = n_obs,y = aisle,fill = n_obs)) +
  geom_col(orientation = "y") +
  xlab("items_saled") +
  viridis::scale_fill_viridis(trans = "log",
                              name = "Items saled",
                              breaks = c(25000,50000,100000)) +
  labs(title = "Popular items with 10000 more sales")
```
One concern for this analysis is that there would be more than 1 of each items within an order. However, the amount of items within an order is not given.\
To make a table showing the three most popular items in these isles, we have\
```{r, message=FALSE}
instacart %>%
  filter(aisle %in% c("baking ingredients",
                      "dog food care",
                      "packaged vegetables fruits"
                      )
         ) %>%
  group_by(aisle, product_name) %>%
  summarise(n_obs = n()) %>%
  arrange(aisle, -n_obs) %>%
  summarise(
    popular_item = head(product_name, 3),
    n = head(n_obs, 3),
    rank = head(rank(-n_obs), 3)
  )
```

And to make a table showing the mean hour of the day, use the following code:\ 
```{r, message=FALSE}
instacart %>%
  filter(product_name %in% c("Pink Lady Apples", "Coffee Ice Cream")) %>%
  group_by(
    product_name,
    order_dow
  ) %>%
  summarise(
    mean_hour = mean(order_hour_of_day)
  ) %>%
  pivot_wider(
    names_from = "order_dow",
    values_from = "mean_hour",
    names_prefix = "day "
  )
```


# Problem 2\
First, we should tidy the dataset:\
```{r}
rm(list = ls())
accel_df = read_csv("data/accel_data.csv") %>%
  pivot_longer(
    activity.1:activity.1440,
    names_to = "minute",
    names_prefix = "activity.",
    values_to = "activity"
  ) %>%
  mutate(
    minute = as.integer(minute),
    weekends = day %in% c("Saturday","Sunday"),
    day = factor(day,levels = c("Sunday", "Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday"))
  ) %>%
  select(day_id, week, day, weekends, minute, activity)
  
head(accel_df,5)
```
Thus, the resulting dataframe `accel_df` consists of $5\times7\times24\times60=50400$ records. Each record contains activity of the male during a minute's period of time, stored in variable `activity`. Other variables in the record shows information for the one-minute time period. \
The following code creates a table for the total activity for each day, and made a plot.\

```{r}
accel_df %>%
  group_by(day_id) %>%
  summarise(total_activity = sum(activity))

accel_df %>%
  group_by(day_id) %>%
  summarise(total_activity = sum(activity)) %>%
  ggplot(aes(x = day_id, y = total_activity)) +
  geom_point() +
  labs(title = "Total activity every day")
```
Thus, no apparant trends can be seen under this analysis, except for some days having almost zero activity.\
However, it is notable that the `day_id` does not represent days after study begins. To solve this problem, use the following code\
```{r}
accel_df %>%
  group_by(week,day) %>%
  summarise(total_activity = sum(activity)) %>%
  mutate(
    day = as.integer(day),
    week = as.factor(week)
  ) %>%
  ggplot(aes(x = day, y = total_activity, color = week)) +
  geom_line() +
  scale_x_continuous(breaks = 1:7,
                     labels = c("Sunday", "Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday")) +
  labs(title = "Total activity every day")

```
From the figure, the activity is more stable from Tuesday to Friday, and have more variance on Sunday and Mondays. The activity on Saturday is not reliable due to missing data for two days.\
However, it is likely that missing data is encoded as 1, thus the `total_activity` here should be lower than the true value. This could be solved by encoding 1 as `NA` and analyze based on `mean(activity)`.\


to make a single-panel plot that shows the 24-hour activity time courses for each day, use the following code: \
```{r}
accel_df %>%
  mutate(day_id = as.factor(day_id),
         week = as.factor(week),
         hour = minute / 60) %>%
  group_by(day_id) %>%
  ggplot(aes(x = hour, y = activity, group = day_id, color = day)) +
  geom_smooth(se = FALSE, method = "loess", span = 0.5) +
  scale_x_continuous(breaks = 0:12 * 2) +
  labs(title = "Activity course every day")
```
 This plot shows that the males tends to exercise in the morning between 5-11 a.m. and in the evening between 18-22 p.m. He seldom move during midnight, and reduced activity at noon.\
 Meanwhile, his exercise on each day across week showed similar pattern. For example, in 4 out of 5 weeks, the man exercised in the evening on Friday (purple lines). \
 Notably, there are negative values in the smoothed lines. However, it does not mean that activity can take negative values.\

# Problem 3
First of all, clean the data using the following code:\
```{r}
rm(list = ls())
data("ny_noaa")
ny_noaa = ny_noaa %>%
  mutate(
    id = as.factor(id),
    year = year(date),
    month = factor(month.name[month(date, label = TRUE)], levels = month.name),
    day = day(date),
    prcp = as.double(prcp) / 10,
    tmax = as.double(tmax) / 10,
    tmin = as.double(tmin) / 10
    ) %>%
  select(id, date, year, month, day, everything()) %>%
  arrange(date)
```
The dataframe consists of `r nrow(ny_noaa)` records from `r nrow(distinct(ny_noaa,id))` weather stations, lasting from ``r ny_noaa$date[1]` to `r tail(ny_noaa$date, 1)`. For each record, there are information for the Precipitation, Snowfall, Snow depth(mm) and Maximum/minimum Temperature$(^oC)$ information.\

We can count the frequency based on the following code:\
```{r}
ny_noaa %>%
  group_by(snow) %>%
  summarise(n_obs = n()) %>%
  arrange(-n_obs) %>%
  head(3)
```
Because most days don't snow, the most frequent value is 0, indicating no snows.\

The code below makes a two-panel plot showing the average max temperature in January and in July. Each line represents a record from a  different station.\
```{r}
ny_noaa %>%
  subset(month %in% c("January", "July")) %>%
  group_by(year, month, id) %>%
  summarise(
    ave_tmax = mean(tmax, na.rm = TRUE),
  ) %>%
  ggplot() +
  geom_line(aes(x = year, y = ave_tmax,group = id, color = month), alpha = 0.5) +
  facet_grid(~month) +
  theme(legend.position = "none") +
  labs(title = "Average maximum temperature in January and July")
```
From the figure, it is obvious that the average maximum temperature in July is always higher than in January, regardless of station and year. Meanwhile, the average maximum temperature from different stations tend to increase or drop together. During cold months, the record of all stations decreases to a similar extent, forming parallel lines in the figure. However, there are some notable outliers not following this pattern, such as the dip in July, 1988.\


The code to make the two panel plot and the result is shown below. Considering the computational efficiency and differences across stations, take the average temperature between stations will have a better plot. \  
```{r}
ny_noaa %>%
  mutate(day_of_year = yday(date),
         year = as.factor(year)) %>%
  group_by(year, day_of_year) %>%
  summarise(ave_tmax = mean(tmax, na.rm = TRUE),
            ave_tmin = mean(tmin, na.rm = TRUE)) %>%
  pivot_longer(ave_tmax:ave_tmin,
               names_to = "t_type",
               values_to = "tempreture",
               names_prefix = "ave_") %>%
  ggplot(aes(x = day_of_year, y = tempreture , group = year, color = t_type)) +
  geom_line(alpha = 0.3) +
  facet_grid(~t_type) +
  labs(title = "Temperature extremes during the year")
```


```{r}
ny_noaa %>%
  filter(snow > 0 & snow < 100) %>%
  mutate(year = as.factor(year)) %>%
  ggplot(aes(x = snow, y = year, fill = year)) +
  geom_density_ridges(scale = 1.5, alpha = 0.5, color = "#0000005F") +
  scale_x_continuous(limit = c(0, 100)) +
  labs(
    title = "snowfall distribution across years",
    x = "snowfall (mm)"
  )
```





